{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport sys\nimport tensorflow.compat.v1 as tf\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nyx = np.array(pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\"))\nX_test = np.array(pd.read_csv('/kaggle/input/digit-recognizer/test.csv'))\nY_train = yx[:, 0]\nY_train = np.transpose(np.matrix([Y_train]))\nX_train = yx[:, 1:]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, Y_train)","execution_count":5,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:179: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  return self._fit(X, y)\n","name":"stderr"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"KNeighborsClassifier(n_neighbors=10)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_y_pred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [np.argmax(x) for x in knn_y_pred]\nids = [x+1 for x in range(len(knn_y_pred))]\n\nsub = pd.DataFrame()\n\nsub['ImageId'] = ids\nsub['Label'] = labels\n\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Learning Curve \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\n  \nx_train = X_train[0:10000]\ny_train = Y_train[0:10000]\n  \n# Obtain scores from learning curve function\n# cv is the number of folds while performing Cross Validation\nsizes, training_scores, testing_scores = learning_curve(knn, x_train, y_train, cv=5, scoring='accuracy')\n  \n# Mean and Standard Deviation of training scores\nmean_training = np.mean(training_scores, axis=1)\nStandard_Deviation_training = np.std(training_scores, axis=1)\n  \n# Mean and Standard Deviation of testing scores\nmean_testing = np.mean(testing_scores, axis=1)\nStandard_Deviation_testing = np.std(testing_scores, axis=1)\n  \n# dotted blue line is for training scores and green line is for cross-validation score\nplt.plot(sizes, mean_training, '--', color=\"b\",  label=\"Training score\")\nplt.plot(sizes, mean_testing, color=\"g\", label=\"Cross-validation score\")\n  \n# Drawing plot\nplt.title(\"LEARNING CURVE FOR KNN Classifier\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Multi Layer Perceptron\n\n\nfrom sklearn.neural_network import MLPClassifier\nmlpClassifier = MLPClassifier(hidden_layer_sizes=100, activation='relu', solver='adam', alpha=0.0001, batch_size='auto', \n             learning_rate='constant', \n              learning_rate_init=0.0009, power_t=0.5, max_iter=250, shuffle=True, random_state=None, tol=0.0001, \n              verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n\n# epsilon=1e-08 Value for numerical stability in adam. Only used when solver=’adam’\nmlpClassifier.fit(X_train, Y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_y_pred = mlpClassifier.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [np.argmax(x) for x in mlp_y_pred]\nids = [x+1 for x in range(len(mlp_y_pred))]\n\nsub = pd.DataFrame()\n\nsub['ImageId'] = ids\nsub['Label'] = labels\n\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Learning Curve for MLP\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes.set_title(title)\n    if ylim is not None:\n        axes.set_ylim(*ylim)\n    axes.set_xlabel(\"Training examples\")\n    axes.set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes.legend(loc=\"best\")\n\n    return plt\n\n\nfig, axes = plt.subplots(1, 1)\n\n\ntitle = \"Learning Curves\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\n#cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nplot_learning_curve(mlpClassifier, title, X_train, Y_train, axes=axes, ylim=(0.7, 1.01)\n                    ,cv=3 \n                    ,n_jobs=1)\n# SVC is more expensive so we do a lower number of CV iterations:\n#plt.savefig(\"MLP_learningCurve.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To get images near decision boundaries\n\n#MLP \n# At image  3  Predicted as 0  with probability  0.5285171679951194 near probability of  9  is  0.4672753588286859\n# At image  24907  Predicted as 1  with probability  0.5103879294802369 near probability of  9  is  0.4675327258484755\n# At image  13200  Predicted as 2  with probability  0.5560217172706002 near probability of  7  is  0.4439782827289965\n# At image  17205  Predicted as 3  with probability  0.5014700056696101 near probability of  8  is  0.4984282708620153\n# At image  15440  Predicted as 4  with probability  0.4995766103290047 near probability of  7  is  0.4984102402548431\n# At image  2297  Predicted as 5  with probability  0.5033862952747804 near probability of  4  is  0.4966137023580073\n# At image  14401  Predicted as 6  with probability  0.4094020376969683 near probability of  5  is  0.2891308549880324\n# At image  429  Predicted as 7  with probability  0.5066073567428863 near probability of  3  is  0.4933914979050861\n# At image  1653  Predicted as 8  with probability  0.5058008324199895 near probability of  3  is  0.4941989510999071\n# At image  3217  Predicted as 9  with probability  0.5028330820221076 near probability of  3  is  0.4971668625742563\n\nMLP_y_pred_probab = mlp.predict_proba(X_test)\n\nMLP_y_pred = np.array(mlp_y_pred)\n\nk=1\nfor k in range(0,1):\n    min = 1\n    min_j = 0\n    min_i=0\n    for i in range(0,len(y_pred_probab)):\n        for j in range(0, len(y_pred_probab[0])):\n            if y_pred[i][1]==k and y_pred_probab[i][j] > 0.4 and y_pred[i][1] != j:\n                diff = y_pred_probab[i][k] - y_pred_probab[i][j]\n                if diff<min:\n                    min = diff\n                    min_i = i\n                    min_j = j\n                    print(\"# At image \",i,\" Predicted as\", y_pred[i][1], \" with probability \", y_pred_probab[i][k], \"near probability of \", j, \" is \", y_pred_probab[i][j])\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}